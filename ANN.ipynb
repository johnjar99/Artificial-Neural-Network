{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Implementación de una red profunda\n"
      ],
      "metadata": {
        "id": "ZA8-xca_PIfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En esta implementación de Deep learning, nuestro objetivo es predecir la rotación de clientes para un determinado\n",
        "banco, y de esta forma saber que clientes tienen mayor probabilidad de andonar el servicio bancario.\n",
        "En conjunto de datos utilizados es relativamente pequeño y contiene 10000 filas con 14 columnas\n",
        "\n",
        "Se está utilizando la distribución Anaconda y marcos como Theano, TensorFlow y Keras.\n",
        "Keras está construido sobre Tensorflow y Theano, que funcionan como backends.\n"
      ],
      "metadata": {
        "id": "r9Q_PUlePOJC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Artificial Neural Network\n",
        " Se comienza isntalando las librerias y cargando los datos."
      ],
      "metadata": {
        "id": "NAzkTTDtP3p7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8osy6WUlOPI4",
        "outputId": "c61345cf-c838-4558-8293-ad1563c7fbdc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting theano\n",
            "  Downloading Theano-1.0.5.tar.gz (2.8 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.8 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.8/2.8 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.10/dist-packages (from theano) (1.26.4)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.10/dist-packages (from theano) (1.13.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from theano) (1.16.0)\n",
            "Building wheels for collected packages: theano\n",
            "  Building wheel for theano (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for theano: filename=Theano-1.0.5-py3-none-any.whl size=2668110 sha256=724554ead88707b5204b94ae6b8e54633247de157b72b0e79556fdbb323b8522\n",
            "  Stored in directory: /root/.cache/pip/wheels/d9/e6/7d/2267d21a99e4ab8276f976f293b4ff23f50c9d809f4a216ebb\n",
            "Successfully built theano\n",
            "Installing collected packages: theano\n",
            "Successfully installed theano-1.0.5\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.31.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (3.4.1)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from keras) (1.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from keras) (1.26.4)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras) (13.7.1)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.10/dist-packages (from keras) (3.11.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras) (0.12.1)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.10/dist-packages (from keras) (0.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from keras) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from optree->keras) (4.12.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# Instalar Theano\n",
        "!pip install theano\n",
        "\n",
        "# Instalar Tensorflow\n",
        "!pip install tensorflow\n",
        "\n",
        "# Instalar Keras\n",
        "!pip install keras\n",
        "\n",
        "# Importar las librerias\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# Importar la base de datos\n",
        "\n",
        "dataset = pd.read_csv('/content/REC017260.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se crea arrays con las características incluidas en el conjunto de datos y la variable objetivo que es la columna 14, etiquetada como \"Exited\""
      ],
      "metadata": {
        "id": "a_Cv9gO9SD4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# El aspecto inicial de los datos es como se muestra a continuación:\n",
        "\n",
        "X = dataset.iloc[:,3:13].values #Db\n",
        "print(X)\n",
        "\n",
        "Y = dataset.iloc[:,13].values #valores de la variable \"Exited\"\n",
        "print(Y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIRAbIy0S4tj",
        "outputId": "f005e6b0-5def-4d9e-9a7c-2dba3263d9dc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[619 'France' 'Female' ... 1 1 101348.88]\n",
            " [608 'Spain' 'Female' ... 0 1 112542.58]\n",
            " [502 'France' 'Female' ... 1 0 113931.57]\n",
            " ...\n",
            " [709 'France' 'Female' ... 0 1 42085.58]\n",
            " [772 'Germany' 'Male' ... 1 0 92888.52]\n",
            " [792 'France' 'Female' ... 1 0 38190.78]]\n",
            "[1 0 1 ... 1 1 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para realizar la codificación one-hot de variables categóricas se puede usar el código que usa la librería scikit-learn. Se importa la clase OneHotEncoder de sklearn.preprocessing y la librería numpy.\n",
        "Se crea un array de ejemplo llamado categorias que contiene variables categóricas A,B,C,A,C. Este array se cnvierte en un matriz de una sola columna utilizando reshape(-1,1) para que tenda una forma adecuada para el procesamiento de OneHotEncoder.\n",
        "Se crea una instancia de OneHotEncoder llamada encoder.\n",
        "Se ajusta y transforma los datos categóricos utilizando el método fit_transform del objetivo encoder. Esto convierte las cetegorías en una representación one-hot, los datos codificados se convierten en una matriz densa (una matriz regular) utilizadno el método toarray().\n",
        "Finalmente, se imprime la matriz densa que contiene los datos codificados.\n"
      ],
      "metadata": {
        "id": "zt--D9SINHzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "import numpy as np\n",
        "\n",
        "# Supongamos que ya tienes el array X definido\n",
        "# Ejemplo: X = np.array([...])\n",
        "\n",
        "# Codificar las columnas específicas usando LabelEncoder\n",
        "labelencoder_X_1 = LabelEncoder()\n",
        "X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])\n",
        "\n",
        "labelencoder_X_2 = LabelEncoder()\n",
        "X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])\n",
        "\n",
        "# Crear el transformador de columnas especificando las columnas categóricas\n",
        "column_transformer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('onehot', OneHotEncoder(), [1, 2])  # Índices de las columnas categóricas\n",
        "    ],\n",
        "    remainder='passthrough'  # Dejar las demás columnas tal cual\n",
        ")\n",
        "\n",
        "# Ajustar y transformar los datos\n",
        "X = column_transformer.fit_transform(X)\n",
        "\n",
        "# Eliminar la primera columna para evitar la trampa de variables ficticias\n",
        "X = X[:, 1:]\n",
        "\n",
        "print(X)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nt_PIuN1STS9",
        "outputId": "21951f65-70f6-49f5-bda1-ea187370bb74"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.0 0.0 1.0 ... 1 1 101348.88]\n",
            " [0.0 1.0 1.0 ... 0 1 112542.58]\n",
            " [0.0 0.0 1.0 ... 1 0 113931.57]\n",
            " ...\n",
            " [0.0 0.0 1.0 ... 0 1 42085.58]\n",
            " [1.0 0.0 0.0 ... 1 0 92888.52]\n",
            " [0.0 0.0 1.0 ... 1 0 38190.78]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se utiliza la función **train_test_split** de ScikitLear para dividir nuestros datos en conjuntos de entrenamiento y conjunto de prueba. Se mantiene el **ratio train_test_split** a la proporción **80:20**\n"
      ],
      "metadata": {
        "id": "9LSRFLInV_l6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dividir el dataset en Training set y Test set\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)\n"
      ],
      "metadata": {
        "id": "R3cPso2aV-90"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se escalan los datos para que sean más representativos.\n",
        "\n",
        "Se ajusta y transforma los datos de entrenamiento usando la función **StandardScaler**.\n",
        "\n",
        "Se estandariza nuestra escala para que se utilice el mismo método ajustado para transformar / escalar datos de prueba.\n"
      ],
      "metadata": {
        "id": "e-tjmxuYWUYj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "sc = StandardScaler()\n",
        "\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.transform(X_test)\n",
        "\n",
        "X\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1OfDk00WiTt",
        "outputId": "39577433-e50e-4c23-e213-97c50a856354"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.0, 0.0, 1.0, ..., 1, 1, 101348.88],\n",
              "       [0.0, 1.0, 1.0, ..., 0, 1, 112542.58],\n",
              "       [0.0, 0.0, 1.0, ..., 1, 0, 113931.57],\n",
              "       ...,\n",
              "       [0.0, 0.0, 1.0, ..., 0, 1, 42085.58],\n",
              "       [1.0, 0.0, 0.0, ..., 1, 0, 92888.52],\n",
              "       [0.0, 0.0, 1.0, ..., 1, 0, 38190.78]], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Los datos ahora se escalan correctamente. Finalmente, ya tenemos listo el preprocesamiento de los datos.\n",
        "\n",
        "Ahora, se comienza con nuestro modelo, se importan los módulos necesarios aquí. Se necesita el módulo **secuencial** para inicializar la red neuronal y el módulo **dense** para agregar las capas ocultas\n"
      ],
      "metadata": {
        "id": "jrdWg0WYWwuK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar la libreria Keras y sus paquetes\n",
        "\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Add a Flatten layer before your Dense layer\n",
        "from keras.layers import Flatten\n"
      ],
      "metadata": {
        "id": "YoBUaJ29XOfI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El nombre del modelo será **\"Classifier\"** ya que nuestro objetivo es clasificar la rotación de clientes.\n",
        "\n",
        "Luego se usa el módulo secuencial para la inicialización\n"
      ],
      "metadata": {
        "id": "aPzGtTTnXU2b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Iniciamos la red neuronal\n",
        "\n",
        "classifier = Sequential()\n",
        "classifier.add(Flatten())"
      ],
      "metadata": {
        "id": "BwCVH2NwXbXa"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se agrega las capas ocultas una por una usando la función **Dense**.\n",
        "\n",
        "Nuesto primer parámetro es **output_dim**. Es la cantidad de nodos que agregamos a esta capa. **int** es la inicialización del gradiente estocástico descendiente.\n"
      ],
      "metadata": {
        "id": "2f69g503Z7Jy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "En una red neuronal se asigna pesos a cada nodo. En la inicialización, los pesos deben estar cerca de cero y los inicialiamos aleatoriamente usando la función **uniform**.\n",
        "\n",
        "El parámetro **input_dim** es necesario solo para la primera capa, ya que el modleo no concoce el úmero de nuestras variablesde enrada.\n",
        "\n",
        "Aquí el número total de variables de entrada es 11. En la segunda capa, el modelo conoce automáticamente el número de variables de entrada de la primera capa oculta.\n"
      ],
      "metadata": {
        "id": "W3xKHFCDaB-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Agregar la capa de entrada y la primera capa oculta:\n",
        "\n",
        "input_dim = X_train.shape[1]  # Asegurarse de que input_dim sea el número correcto de características\n",
        "classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=input_dim))\n",
        "\n",
        "# Se agrega la segunda capa oculta:\n",
        "\n",
        "classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
        "\n",
        "# Se agrega la capa de salida:\n",
        "\n",
        "classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yWEw5Ks9aQLT",
        "outputId": "33bb495d-05cf-44db-ca49-de24586bce3e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora se va a compilar nuestra red. Hasta ahora se ha agregado varias capas a nuestro clasificador.\n",
        "\n",
        "Ahora se va a ir compilando utilizando el método de **compile**. Los argumentos agregados en el control de compilación final completan la red neuronal, por lo que se debe ser cuidadosos en este paso.\n"
      ],
      "metadata": {
        "id": "5457SWBuanu4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optimizer** Conjunto óptimo de pesos. **Denscenso de gradiente estocático (SGD)**. El SGD depende de la perdida por lo que nuesto segundo parámetro es la pérdida. Si nuestra variable dependiente es binaria, se usa la función de perdida logaritmica llamada **'binary_crossentropy'**, y si nuestra variable dependiente tiene más de dos categorias\n",
        "en la salida, entonces usamos **'categorical_crossentropy'**.\n",
        "\n",
        "Se quiere mejorar el rendimiento de nuestra red neuronal en función de la precisión por lo que agregamos métricas como precisión.\n"
      ],
      "metadata": {
        "id": "jv3JoHxraumT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compilar red neuronal\n",
        "\n",
        "classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n"
      ],
      "metadata": {
        "id": "I0MIakzjanaU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Se deben ajustar varios códigos en este paso**\n",
        "\n",
        "Ajuste de la red neuronal al conjunto de entrenamiento\n",
        "\n",
        "Ahora se entrena nuestro modelo en los datos de entrenamiento. Se usa el método **fit** para adaptarse a nuestro modelo.\n",
        "\n",
        "También se optimiza los pesos para mejorar la eficiencia del modelo. Para esto, se tiene que actualizar los pesos.\n",
        "\n",
        "El **Tamaño del lote (Batch size)** es el número de observaciones después de las cuales actualizamos los pesos.\n",
        "\n",
        "La **Época (epoch)** es el número total de iteraciones. Los valores de tamaño de lote y época se eligen por el método de prueba y error.\n"
      ],
      "metadata": {
        "id": "ohPCdIUDbM3S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier.fit(X_train, y_train, batch_size = 10, epochs = 50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z411aIh2bccf",
        "outputId": "08637ec9-d121-4046-a753-4f6522d313da"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step - accuracy: 0.7919 - loss: 0.5634\n",
            "Epoch 2/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7987 - loss: 0.4249\n",
            "Epoch 3/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.7909 - loss: 0.4247\n",
            "Epoch 4/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8087 - loss: 0.4192\n",
            "Epoch 5/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8237 - loss: 0.4278\n",
            "Epoch 6/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8342 - loss: 0.4070\n",
            "Epoch 7/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8284 - loss: 0.4157\n",
            "Epoch 8/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.8289 - loss: 0.4146\n",
            "Epoch 9/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8387 - loss: 0.4070\n",
            "Epoch 10/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8379 - loss: 0.3984\n",
            "Epoch 11/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8303 - loss: 0.4113\n",
            "Epoch 12/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8373 - loss: 0.4056\n",
            "Epoch 13/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8331 - loss: 0.4052\n",
            "Epoch 14/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8338 - loss: 0.4055\n",
            "Epoch 15/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8302 - loss: 0.4168\n",
            "Epoch 16/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8337 - loss: 0.4140\n",
            "Epoch 17/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8297 - loss: 0.4200\n",
            "Epoch 18/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8435 - loss: 0.3949\n",
            "Epoch 19/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8345 - loss: 0.4079\n",
            "Epoch 20/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8378 - loss: 0.3984\n",
            "Epoch 21/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8348 - loss: 0.4010\n",
            "Epoch 22/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8356 - loss: 0.4089\n",
            "Epoch 23/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8394 - loss: 0.3950\n",
            "Epoch 24/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8399 - loss: 0.3949\n",
            "Epoch 25/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8362 - loss: 0.4074\n",
            "Epoch 26/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8337 - loss: 0.4061\n",
            "Epoch 27/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8385 - loss: 0.3985\n",
            "Epoch 28/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8356 - loss: 0.4061\n",
            "Epoch 29/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8336 - loss: 0.4109\n",
            "Epoch 30/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8345 - loss: 0.4046\n",
            "Epoch 31/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8412 - loss: 0.3938\n",
            "Epoch 32/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8312 - loss: 0.4152\n",
            "Epoch 33/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8407 - loss: 0.3948\n",
            "Epoch 34/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8403 - loss: 0.3958\n",
            "Epoch 35/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.8327 - loss: 0.4072\n",
            "Epoch 36/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step - accuracy: 0.8366 - loss: 0.3962\n",
            "Epoch 37/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8350 - loss: 0.3983\n",
            "Epoch 38/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8339 - loss: 0.4053\n",
            "Epoch 39/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8371 - loss: 0.4023\n",
            "Epoch 40/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8421 - loss: 0.3922\n",
            "Epoch 41/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8383 - loss: 0.3928\n",
            "Epoch 42/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8386 - loss: 0.3989\n",
            "Epoch 43/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8400 - loss: 0.3903\n",
            "Epoch 44/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.8423 - loss: 0.3858\n",
            "Epoch 45/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8387 - loss: 0.3933\n",
            "Epoch 46/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8308 - loss: 0.4081\n",
            "Epoch 47/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8377 - loss: 0.4029\n",
            "Epoch 48/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8300 - loss: 0.4088\n",
            "Epoch 49/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8340 - loss: 0.4065\n",
            "Epoch 50/50\n",
            "\u001b[1m800/800\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.8370 - loss: 0.3968\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7bf345bcf8e0>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ya se está en condiciones de hacer predicciones y evaluar el modelo."
      ],
      "metadata": {
        "id": "tGOEaPJBgkHD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Predecir los resultados Test set\n",
        "\n",
        "y_pred = classifier.predict(X_test)\n",
        "y_pred = (y_pred > 0.5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pk4B_qleT0s",
        "outputId": "207d1528-0c56-4e9c-da78-f06b4ff8b13f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m63/63\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado de la predicción le dará la probabilidad de que el cliente abandole la empresa. Se convierte esa probabilidad en binario 0 y 1.\n",
        "\n",
        "Se va a predecir sobre una observación:\n",
        "\n",
        "Geography: Spain\n",
        "\n",
        "Credit Score: 500\n",
        "\n",
        "Gender: Female\n",
        "\n",
        "Age: 40\n",
        "\n",
        "Tenure: 3\n",
        "\n",
        "Balance: 50000\n",
        "\n",
        "Number of Products: 2\n",
        "\n",
        "Has Credit Card: Yes\n",
        "\n",
        "Is Active Member: Yes"
      ],
      "metadata": {
        "id": "h1asF5eVhND4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Todos estos valores se codifican, según vimos antes:\n",
        "\n",
        "new_prediction = classifier.predict(sc.transform(np.array([[0,0.0,0,500,1,40,3,50000,2,1,1,40000]])))\n",
        "new_prediction = (new_prediction > 0.5)\n",
        "new_prediction"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4jpAMut3hcXp",
        "outputId": "4537680c-0e8e-449d-d8c8-526c64e4c6e9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 95ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[False]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el último paso evaluamos el rendimiento de nuestro modelo. Ya se tienen resultados originales y, por lo tanto, se puede construir una matriz de confusión para verificar la precisión del modelo.\n"
      ],
      "metadata": {
        "id": "8kgCbAQiieik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Se contruye la matriz de confusión:\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "cm = confusion_matrix(y_test,y_pred)\n",
        "cm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qt6B9QQiiHA",
        "outputId": "df4ea841-6de2-4094-ca30-882dc41a0378"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1537,   57],\n",
              "       [ 285,  121]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# A partir de la matriz de confusión, la precisión de nuestro modelo se puede clacular co\n",
        "import numpy as np\n",
        "\n",
        "# Extraer los valores\n",
        "posicion1 = cm[0, 0]\n",
        "posicion4 = cm[1, 1]\n",
        "\n",
        "# Realizar el cálculo\n",
        "resultado = ((posicion1 + posicion4) / 2000)*100\n",
        "\n",
        "# Mostrar el resultado\n",
        "print(\"El resultado es:\", resultado)\n",
        "\n",
        "# Se puede ver el resultado de la predicción con:\n",
        "\n",
        "print(new_prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW4BZ99Ui_TJ",
        "outputId": "52b72953-dedc-4152-d610-c5e45cff4e1e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El resultado es: 82.89999999999999\n",
            "[[False]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado es que el cliente no nos abandona."
      ],
      "metadata": {
        "id": "cSXrzBB-jSQ-"
      }
    }
  ]
}