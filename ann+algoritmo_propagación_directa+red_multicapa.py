# -*- coding: utf-8 -*-
"""ANN+Algoritmo propagación directa+Red multicapa.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1GH0Z_SAS5Yekp1O6UAqIpQ0wWDKT9_We

# Implementación de una red profunda

En esta implementación de Deep learning, nuestro objetivo es predecir la rotación de clientes para un determinado
banco, y de esta forma saber que clientes tienen mayor probabilidad de andonar el servicio bancario.
En conjunto de datos utilizados es relativamente pequeño y contiene 10000 filas con 14 columnas

Se está utilizando la distribución Anaconda y marcos como Theano, TensorFlow y Keras.
Keras está construido sobre Tensorflow y Theano, que funcionan como backends.

# Artificial Neural Network
 Se comienza isntalando las librerias y cargando los datos.
"""

# Instalar Theano
!pip install theano

# Instalar Tensorflow
!pip install tensorflow

# Instalar Keras
!pip install keras

# Importar las librerias

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd

# Importar la base de datos

dataset = pd.read_csv('/content/REC017260.csv')

"""Se crea arrays con las características incluidas en el conjunto de datos y la variable objetivo que es la columna 14, etiquetada como "Exited"
"""

# El aspecto inicial de los datos es como se muestra a continuación:

X = dataset.iloc[:,3:13].values #Db
print(X)

Y = dataset.iloc[:,13].values #valores de la variable "Exited"
print(Y)

"""Para realizar la codificación one-hot de variables categóricas se puede usar el código que usa la librería scikit-learn. Se importa la clase OneHotEncoder de sklearn.preprocessing y la librería numpy.
Se crea un array de ejemplo llamado categorias que contiene variables categóricas A,B,C,A,C. Este array se cnvierte en un matriz de una sola columna utilizando reshape(-1,1) para que tenda una forma adecuada para el procesamiento de OneHotEncoder.
Se crea una instancia de OneHotEncoder llamada encoder.
Se ajusta y transforma los datos categóricos utilizando el método fit_transform del objetivo encoder. Esto convierte las cetegorías en una representación one-hot, los datos codificados se convierten en una matriz densa (una matriz regular) utilizadno el método toarray().
Finalmente, se imprime la matriz densa que contiene los datos codificados.

"""

from sklearn.preprocessing import OneHotEncoder, LabelEncoder
from sklearn.compose import ColumnTransformer
import numpy as np

# Supongamos que ya tienes el array X definido
# Ejemplo: X = np.array([...])

# Codificar las columnas específicas usando LabelEncoder
labelencoder_X_1 = LabelEncoder()
X[:, 1] = labelencoder_X_1.fit_transform(X[:, 1])

labelencoder_X_2 = LabelEncoder()
X[:, 2] = labelencoder_X_2.fit_transform(X[:, 2])

# Crear el transformador de columnas especificando las columnas categóricas
column_transformer = ColumnTransformer(
    transformers=[
        ('onehot', OneHotEncoder(), [1, 2])  # Índices de las columnas categóricas
    ],
    remainder='passthrough'  # Dejar las demás columnas tal cual
)

# Ajustar y transformar los datos
X = column_transformer.fit_transform(X)

# Eliminar la primera columna para evitar la trampa de variables ficticias
X = X[:, 1:]

print(X)

"""Se utiliza la función **train_test_split** de ScikitLear para dividir nuestros datos en conjuntos de entrenamiento y conjunto de prueba. Se mantiene el **ratio train_test_split** a la proporción **80:20**

"""

# Dividir el dataset en Training set y Test set

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(X,Y, test_size=0.2)

"""Se escalan los datos para que sean más representativos.

Se ajusta y transforma los datos de entrenamiento usando la función **StandardScaler**.

Se estandariza nuestra escala para que se utilice el mismo método ajustado para transformar / escalar datos de prueba.

"""

# Feature Scaling

from sklearn.preprocessing import StandardScaler

sc = StandardScaler()

X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

X

"""Los datos ahora se escalan correctamente. Finalmente, ya tenemos listo el preprocesamiento de los datos.

Ahora, se comienza con nuestro modelo, se importan los módulos necesarios aquí. Se necesita el módulo **secuencial** para inicializar la red neuronal y el módulo **dense** para agregar las capas ocultas

"""

# Importar la libreria Keras y sus paquetes

import keras
from keras.models import Sequential
from keras.layers import Dense

# Add a Flatten layer before your Dense layer
from keras.layers import Flatten

"""El nombre del modelo será **"Classifier"** ya que nuestro objetivo es clasificar la rotación de clientes.

Luego se usa el módulo secuencial para la inicialización

"""

# Iniciamos la red neuronal

classifier = Sequential()
classifier.add(Flatten())

"""Se agrega las capas ocultas una por una usando la función **Dense**.

Nuesto primer parámetro es **output_dim**. Es la cantidad de nodos que agregamos a esta capa. **int** es la inicialización del gradiente estocástico descendiente.

En una red neuronal se asigna pesos a cada nodo. En la inicialización, los pesos deben estar cerca de cero y los inicialiamos aleatoriamente usando la función **uniform**.

El parámetro **input_dim** es necesario solo para la primera capa, ya que el modleo no concoce el úmero de nuestras variablesde enrada.

Aquí el número total de variables de entrada es 11. En la segunda capa, el modelo conoce automáticamente el número de variables de entrada de la primera capa oculta.
"""

# Agregar la capa de entrada y la primera capa oculta:

input_dim = X_train.shape[1]  # Asegurarse de que input_dim sea el número correcto de características
classifier.add(Dense(units=6, kernel_initializer='uniform', activation='relu', input_dim=input_dim))

# Se agrega la segunda capa oculta:

classifier.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))

# Se agrega la capa de salida:

classifier.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))

"""Ahora se va a compilar nuestra red. Hasta ahora se ha agregado varias capas a nuestro clasificador.

Ahora se va a ir compilando utilizando el método de **compile**. Los argumentos agregados en el control de compilación final completan la red neuronal, por lo que se debe ser cuidadosos en este paso.

**Optimizer** Conjunto óptimo de pesos. **Denscenso de gradiente estocático (SGD)**. El SGD depende de la perdida por lo que nuesto segundo parámetro es la pérdida. Si nuestra variable dependiente es binaria, se usa la función de perdida logaritmica llamada **'binary_crossentropy'**, y si nuestra variable dependiente tiene más de dos categorias
en la salida, entonces usamos **'categorical_crossentropy'**.

Se quiere mejorar el rendimiento de nuestra red neuronal en función de la precisión por lo que agregamos métricas como precisión.
"""

# Compilar red neuronal

classifier.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])

"""**Se deben ajustar varios códigos en este paso**

Ajuste de la red neuronal al conjunto de entrenamiento

Ahora se entrena nuestro modelo en los datos de entrenamiento. Se usa el método **fit** para adaptarse a nuestro modelo.

También se optimiza los pesos para mejorar la eficiencia del modelo. Para esto, se tiene que actualizar los pesos.

El **Tamaño del lote (Batch size)** es el número de observaciones después de las cuales actualizamos los pesos.

La **Época (epoch)** es el número total de iteraciones. Los valores de tamaño de lote y época se eligen por el método de prueba y error.

"""

classifier.fit(X_train, y_train, batch_size = 10, epochs = 50)

"""# Ya se está en condiciones de hacer predicciones y evaluar el modelo."""

# Predecir los resultados Test set

y_pred = classifier.predict(X_test)
y_pred = (y_pred > 0.5)

"""El resultado de la predicción le dará la probabilidad de que el cliente abandole la empresa. Se convierte esa probabilidad en binario 0 y 1.

Se va a predecir sobre una observación:

Geography: Spain

Credit Score: 500

Gender: Female

Age: 40

Tenure: 3

Balance: 50000

Number of Products: 2

Has Credit Card: Yes

Is Active Member: Yes
"""

# Todos estos valores se codifican, según vimos antes:

new_prediction = classifier.predict(sc.transform(np.array([[0,0.0,0,500,1,40,3,50000,2,1,1,40000]])))
new_prediction = (new_prediction > 0.5)
new_prediction

"""En el último paso evaluamos el rendimiento de nuestro modelo. Ya se tienen resultados originales y, por lo tanto, se puede construir una matriz de confusión para verificar la precisión del modelo.

"""

# Se contruye la matriz de confusión:

from sklearn.metrics import confusion_matrix

cm = confusion_matrix(y_test,y_pred)
cm

# A partir de la matriz de confusión, la precisión de nuestro modelo se puede clacular co
import numpy as np

# Extraer los valores
posicion1 = cm[0, 0]
posicion4 = cm[1, 1]

# Realizar el cálculo
resultado = ((posicion1 + posicion4) / 2000)*100

# Mostrar el resultado
print("El resultado es:", resultado)

# Se puede ver el resultado de la predicción con:

print(new_prediction)

"""El resultado es que el cliente no nos abandona.

#**El algoritmo de propagación directa**

En esta sección se aprendera a cómo escribir código para hacer propagación hacia adelante (predicción) para una red neuronal simple.

En el algoritmo de propagación directa, cada punto de datos es un cliente.
La primera entrada es cuántas cuentas tienen, y la segunda entrada es cuántos hijos tienen.
El modelo predecirá cuántas transacciones realizará el usuario en el próximo año.

Los datos de entrada se cargan preciamente como datos de entreda, y los pesos están en un diccionario llamado **weights**.

El array de pesos para el primer nodo en la capa oculta está en **pesos['node_0']**, y para el segundo nodo en la capa oculta está en **pesos['node_1']** respectivamente.

Los pesos que se alimentan al nodo de salida están disponibles en weights.
Una función de activación es una función que funciona en cada nodo. Transforma la entrada del nodo en alguna salida.

La función de activación lineal rectificada (llamada **ReLU**) se usa ampliamente en redes de muy alto rendimiento.

Ejemplo:

relu(4)=4

relu(-2)=0

Se completa la definición de la función relu():

-Usamos la función **max()** para calcular el valor de la salida de relu().

-Aplicamos la función relu() a **node_0_input** para calcular **node_0_output**.
"""

import numpy as np
input_data = np.array([-1,2])
weights = {
'node_0':np.array([3,3]),
'node_1':np.array([1,5]),
'output':np.array([2,-1])
}

node_0_input = (input_data*weights['node_0']).sum()
node_0_output = np.tanh(node_0_input)

node_1_input = (input_data*weights['node_1']).sum()
node_1_output = np.tanh(node_1_input)

hidden_layer_output = np.array(node_0_output, node_1_output)

output = (hidden_layer_output*weights['output']).sum()
output

"""**Definición de relu función de activación**"""

import numpy as np

# Definir la función relu
def relu(input):
    # Calcular el valor de salida de la función relu: output
    output = max(input, 0)
    # Retornar el valor recién calculado
    return output

# Calcular valor de node 0: node_0_output
node_0_input = (input_data * weights['node_0']).sum()
node_0_output = relu(node_0_input)

# Calcular valor de node 1: node_1_output
node_1_input = (input_data * weights['node_1']).sum()
node_1_output = relu(node_1_input)

# Guardar valores en el array: hidden_layer_outputs
hidden_layer_outputs = np.array([node_0_output, node_1_output])

# Calcular salida del modelo (sin aplicar relu)
model_output = (hidden_layer_outputs * weights['output']).sum()

# Mostrar salida del modelo
print("La salida del modelo es:", model_output)

"""A continuación, se va a desarrollar una función llamada **predic_with_network()**.
Esta función generará predicciones para múltiples observacipnes de datos, tomadas de la red anterior tomada como **input_data**.
Se están utilizando los pesos dados en la red anterior, la función relu() también se está utilizando.

"""

# Definir la función predict_with_network
def predict_with_network(input_data_row, weights):
    # Calcular valor de node 0
    node_0_input = (input_data_row * weights['node_0']).sum()
    node_0_output = relu(node_0_input)

    # Calcular valor de node 1
    node_1_input = (input_data_row * weights['node_1']).sum()
    node_1_output = relu(node_1_input)

    # Guardar los valores de los nodos en un array: hidden_layer_outputs
    hidden_layer_outputs = np.array([node_0_output, node_1_output])

    # Calcular salida del modelo
    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()
    model_output = relu(input_to_final_layer)

    # Retornar salida del modelo
    return model_output

# Crear lista vacía para almacenar los resultados de la predicción
results = []

# Realizar predicciones y almacenarlas en results
for input_data_row in input_data:
    # Agregar predicción a los resultados
    results.append(predict_with_network(input_data_row, weights))

# Imprimir los resultados
print(results)

"""#**Redes Profundas Multicapa**"""

# Ejemplo:
input_data = np.array([3, 5])
weights = {
    'node_0_0': np.array([2,4]),
    'node_0_1': np.array([4,5]),
    'node_1_0': np.array([-1,1]),
    'node_1_1': np.array([2,2]),
    'output': np.array([2,7])
}

# Definir la función predict_with_network
def predict_with_network(input_data_row, weights):
    # Calcular valor de node 0 primera capa oculta
    node_0_0_input = (input_data_row * weights['node_0_0']).sum()
    node_0_0_output = relu(node_0_0_input)

    # Calcular valor de node 1 primera capa oculta
    node_0_1_input = (input_data_row * weights['node_0_1']).sum()
    node_0_1_output = relu(node_0_1_input)

    # Guardar los valores de los nodos en un array: hidden_0_outputs
    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])

    # Calcular valor de node 0 segunda capa oculta
    node_1_0_input = (input_data_row * weights['node_1_0']).sum()
    node_1_0_output = relu(node_1_0_input)

    # Calcular valor de node 1 segunda capa oculta
    node_1_1_input = (input_data_row * weights['node_1_1']).sum()
    node_1_1_output = relu(node_1_1_input)

    # Guardar los valores de los nodos en un array: hidden_1_outputs
    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])

    # Calcular salida del modelo: model output

    model_output = (hidden_1_outputs*weights['output']).sum()

    # Retornar salida del modelo
    return(model_output)

# Crear lista vacía para almacenar los resultados de la predicción
results_m = []

# Realizar predicciones y almacenarlas en results
for input_data_row in input_data:
    # Agregar predicción a los resultados
    results_m.append(predict_with_network(input_data_row, weights))

# Imprimir los resultados
print(results_m)